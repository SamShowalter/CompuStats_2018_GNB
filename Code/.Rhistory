data = master_preprocessing(iris,key, 0.4)
data
#Import test data
library(datasets)
data("iris")
#MUST PROVIDE OWN KEY
key = c("virginica", "versicolor", "setosa")
#Split the data into train and test samples
train_test_split = function(data, test_ratio)
{
sample_nums = 1:nrow(data)
#Pick random number of indices for the test rows
#print(rownames(iris))
test_row_nums = sample(sample_nums, test_ratio*nrow(data), replace = FALSE, prob = NULL)
#Get testing and training rows
test_rows = data[test_row_nums,]
train_rows = data[-test_row_nums,]
# print(nrow(test_rows))
# print(" ")
# print(nrow(train_rows))
return(list("train" = train_rows, "test" = test_rows))
}
#Make a label for each class in a master data object
#Prediction label must be the last column
master_preprocessing = function(data, key, test_ratio)
{
#Get label text of last column
label_text = data[,length(data)]
#Delete the last colum and rename it tempclass
data = data[,-length(data)]
data$tempclass = label_text
#Create final class data
for (key_item in 1:length(key))
{
data$class[data$tempclass == key[key_item]] = key_item
}
#drop tempclass data after use
data = subset(data, select = -c(tempclass))
#Split the data to train and test
train_test = train_test_split(data, test_ratio)
#Get train and test subsets
train = train_test$train
test = train_test$test
#Return a list of all the data information
return(list("train" = train, "test" = test, "key" = key))
}
gaussProb = function(mean,stddev,x)
{
#Get numerator and denominator for pdf of normal dist
#Can also be done with punif
num = exp(-((x - mean)**2) / (2*(stddev**2)))
den = stddev*sqrt(2*pi)
#Return the predictions
if (den == 0) {return(0)}
return(num / den)
}
#Separates records in the training dataset by class (label).
#This takes an input of a training dataset and outputs a dictionary of records by class.
class_sep = function(train, key) {
class_dict = vector(mode="list", length=length(key))
names(class_dict) = seq(1, length(key))
#print(class_dict)
for (i in 1 : length(key)) {
class_dict[[i]] = train[train[, length(train)] == i, - length(train)]
}
return (class_dict)
}
data = master_preprocessing(iris,key, 0.4)
data
sep_by_class = class_sep(data$train, data$key)
sep_by_class
#Function for Log likelihood
log_lik = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + (1 + log((data[i] - theta)^2 / 25))
}
return(sum*-1)
}
plot(theta,exp(log_lik(theta)), type = "l")
#plot function on a fine grid
theta = seq(0,50,0.01)
plot(theta,exp(log_lik(theta)), type = "l")
plot(theta,log_lik(theta), type = "l")
#Function for Log likelihood
log_lik = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + (log(1 + (data[i] - theta)^2 / 25))
}
return(sum*-1)
}
plot(theta,log_lik(theta), type = "l")
#Function for Log likelihood
log_deriv = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + ((2*(data[i] - theta))/ (25 + (data[i] - theta)^2))
}
return(sum)
}
#Runs on a while loop
newton.raphson = function(f, a, b, tol = 1e-5) {
timeout_count = 0
require(numDeriv) # Package for computing f'(x)
x0 = a # Set lower bound
k = 0 # Initialize vector for iteration results
# Check lower bound for zero
fa = f(a)
if (fa == 0.0) {
return(list('root' = a, 'approximations' = NULL))
}
#Check upper bound for zero
fb = f(b)
if (fb == 0.0) {
return(list('root' = b, 'approximations' = NULL))
}
while(T) {
timeout_count = timeout_count + 1
dx = genD(func = f, x = x0)$D[1] # First-order derivative f'(x0)
x1 = x0 - (f(x0) / dx) # Calculate next approximation
k[timeout_count] = x1 # Store approximation
# Once dif is small end the loop
if (abs(x1 - x0) < tol) {
root.approx = tail(k, n=1)
res = list('root' = root.approx, 'approximations' = k)
return(res)
}
# If result has not converged yet
x0 = x1
if(timeout_count > 1500)
{
print("Optimum not reached")
return(0)
}
}
print('Not enough iterations in method')
}
secant.opt(log_deriv,0,50)
#Bisection method
bisection = function(fxn, lower, upper, iter = 100)
{
output = c()
count = 0
while(T)
{
count = count + 1
x = (lower + upper) /2
if (fxn(x) < 0)
{
lower = x
}
else if (fxn(x) > 0)
{
upper = x
}
output = rbind(output, c(count,x))
if(abs((output[length(output)]) - output[length(output)-1]) < 0.00001)
{
return(output)
}
}
}
#Secant optimization
secant.opt = function(f, a, b, tol = 1e-9)
{
output = c()
n = 0
while (T)
{
n = n + 1
x2 = b - f(b) / ((f(b) - f(a)) / (b - a)) # Calculate the new x value
output = rbind(output, c(n,x2))
if (abs(x2 - b) < tol)
{
# Once dif is small end the loop
return(list("root" = x2, "output" = output))
}
# If the root was not determined in the previous iteration, update the values and proceed to the next iteration.
a = b
b = x2
if (n > 1000)
{
print("Error - Did not converge")
return()
}
}
}
secant.opt(log_deriv,20,50)
plot(theta,log_lik(theta), type = "l")
plot(theta,log_deriv(theta), type = "l")
#Get result from secant method
#newton.raphson(log_lik, -20,200)
bisection(log_lik,20,40)
secant.opt(log_deriv,20,40)
secant.opt(log_deriv,20,40)
#Get result from secant method
#newton.raphson(log_lik, -20,200)
bisection(log_lik,20,40)
#Get result from secant method
#newton.raphson(log_lik, -20,200)
newton.raphson(log_lik,20,40)
#Include numDerive package
library(numDeriv)
#Get result from secant method
#newton.raphson(log_lik, -20,200)
newton.raphson(log_lik,20,40)
#Get result from secant method
secant.opt(log_deriv,20,40)
#Include numDerive package
library(numDeriv)
#Function for Log likelihood
log_lik = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + (log(1 + (data[i] - theta)^2 / 25))
}
return(sum*-1)
}
#Function for Log likelihood
log_deriv = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + ((2*(data[i] - theta))/ (25 + (data[i] - theta)^2))
}
return(sum)
}
#Secant optimization
secant.opt = function(f, a, b, tol = 1e-9)
{
output = c()
n = 0
while (T)
{
n = n + 1
x2 = b - f(b) / ((f(b) - f(a)) / (b - a)) # Calculate the new x value
output = rbind(output, c(n,x2))
if (abs(x2 - b) < tol)
{
# Once dif is small end the loop
return(list("root" = x2, "output" = output))
}
# If the root was not determined in the previous iteration, update the values and proceed to the next iteration.
a = b
b = x2
if (n > 1000)
{
print("Error - Did not converge")
return()
}
}
}
#plot log likelihood function on a fine grid
theta = seq(20,40,0.01)
plot(theta,log_lik(theta), type = "l")
#Plot the derivative of the log-likelihood
plot(theta,log_deriv(theta), type = "l")
#Get result from secant method and show on graph
result = secant.opt(log_deriv,20,40)
plot(theta,log_lik(theta), type = "l")
abline(v = result1$root, col = "red")
#Include numDerive package
library(numDeriv)
#Function for Log likelihood
log_lik = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + (log(1 + (data[i] - theta)^2 / 25))
}
return(sum*-1)
}
#Function for Log likelihood
log_deriv = function(theta){
data = c(27,24,33,33,29,36,28,27,31,26,30,41,23,29,34,38,36,29,26,24)
sum = 0
for (i in 1:length(data))
{
sum = sum + ((2*(data[i] - theta))/ (25 + (data[i] - theta)^2))
}
return(sum)
}
#Secant optimization
secant.opt = function(f, a, b, tol = 1e-9)
{
output = c()
n = 0
while (T)
{
n = n + 1
x2 = b - f(b) / ((f(b) - f(a)) / (b - a)) # Calculate the new x value
output = rbind(output, c(n,x2))
if (abs(x2 - b) < tol)
{
# Once dif is small end the loop
return(list("root" = x2, "output" = output))
}
# If the root was not determined in the previous iteration, update the values and proceed to the next iteration.
a = b
b = x2
if (n > 1000)
{
print("Error - Did not converge")
return()
}
}
}
#plot log likelihood function on a fine grid
theta = seq(20,40,0.01)
plot(theta,log_lik(theta), type = "l")
#Plot the derivative of the log-likelihood
plot(theta,log_deriv(theta), type = "l")
#Get result from secant method and show on graph
result = secant.opt(log_deriv,20,40)
plot(theta,log_lik(theta), type = "l")
abline(v = result$root, col = "red")
abline(h = 0, col = "red")
paste("The MLE estimation for this function is", result$root)
data = master_preprocessing(iris,key, 0.4)
data
sep_by_class = class_sep(data$train, data$key)
sep_by_class
class_stats_dict = class_stats(sep_by_class)
class_stats_dict
class_stats_dict = class_stats(sep_by_class)
#Creates a dictionary of the mean and standard deviation of every feature for each class.
#This takes a dictionary of separated training data, and outputs a dictionary of summary statistics
#(mean and stdev). For a dataset with three  classes and four features, there would be a total of
#3x4 = 12 means and stdevs, separated by class.
class_stats = function(train_class_dict) {
class_stats_dict = vector(mode="list", length=length(train_class_dict))
names(class_stats_dict) = seq(1, length(train_class_dict))
for (i in 1 : length(train_class_dict)) {
#train_class_dict["mean"]
for (name in names(train_class_dict[[i]])) {
# add two additional rows to each sub-list
train_class_dict[[i]]["mean", name] = mean(train_class_dict[[i]][, name], na.rm=T) # returns NA if not specify na.rm=T
train_class_dict[[i]]["sd", name] = sd(train_class_dict[[i]][, name], na.rm=T)
}
# assign the two additional rows to appropriate sub-list in `class_stats_dict`
class_stats_dict[[i]] = train_class_dict[[i]][c("mean", "sd"), ]
}
return (class_stats_dict)
}
class_stats_dict = class_stats(sep_by_class)
class_stats_dict
